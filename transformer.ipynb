{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fudan\\Desktop\\pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch, os, math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, Tensor\n",
    "from transformers import BertTokenizer # type: ignore\n",
    "path = os.getcwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "    \n",
    "    def encode(self, text:str)->list:\n",
    "        return self.tokenizer(text, return_tensors='pt')['input_ids'][0].tolist()\n",
    "    \n",
    "    def decode(self, tokens:list)->str:\n",
    "        return self.tokenizer.decode(tokens)\n",
    "\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Dataset(Dataset):\n",
    "    def __init__(self, dir:str):\n",
    "        self.tokens = ''\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.data = self.read_file(dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def one_hot(self, tokens:list, value:float):\n",
    "        res = []\n",
    "        for i in tokens:\n",
    "            vec = torch.full([self.vocab_size], (1-value)/self.vocab_size)\n",
    "            vec[i] = value\n",
    "            res.append(vec)\n",
    "        return torch.stack(res)\n",
    "\n",
    "    def read_file(self, dir: str):\n",
    "        data = []\n",
    "        with open(dir, 'r') as file:\n",
    "            self.tokens = tokenizer.encode(file.read())\n",
    "            x = self.one_hot(self.tokens[:-1], 1)\n",
    "            y = self.one_hot(self.tokens[1:], 1)\n",
    "            data.append((x, y))\n",
    "        return data\n",
    "    \n",
    "data = Transformer_Dataset(path+'Genshin Impact.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(nn.Module):\n",
    "    def __init__(self, d, dk): \n",
    "        # d 是词向量维度，dk 是映射后的维度\n",
    "        super().__init__()\n",
    "        self.dk = dk\n",
    "        self.q = nn.Linear(d, dk)\n",
    "        self.k = nn.Linear(d, dk)\n",
    "        self.v = nn.Linear(d, dk)\n",
    "\n",
    "    def attention(self, Q:Tensor, K:Tensor, V:Tensor, mask:Tensor):\n",
    "        return torch.softmax((Q @ K.transpose(-2, -1)) / self.dk**0.5 + mask, dim = -1) @ V\n",
    "        \n",
    "    def forward(self, x:tuple):\n",
    "        x, mask = x\n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x)\n",
    "        return self.attention(Q, K, V, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, head_num, d, dk, dff):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList()    # 多头注意力机制\n",
    "        for _ in range(head_num):\n",
    "            self.heads.append(self_attention(d, dk))\n",
    "        self.o = nn.Linear(head_num*dk, d)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ffn = nn.Sequential(   # Position-wise Feed-Forward Networks\n",
    "            nn.Linear(d, dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff, d),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "    \n",
    "    def forward(self, x:tuple):\n",
    "        x, mask = x\n",
    "        heads_res = []\n",
    "        for head in self.heads: # 可以并行\n",
    "            heads_res.append(head((x, mask)))\n",
    "        a = self.o(torch.concat(heads_res, dim = -1))\n",
    "        b = self.norm1(a+x)\n",
    "        y = self.norm2(self.ffn(b)+b)   # add & norm\n",
    "        return (y, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer(nn.Module):\n",
    "    def __init__(self, decoder_num=6, head_num=8, d=512, dk=64, dff=2048):\n",
    "        super().__init__()\n",
    "        self.mask = Tensor()\n",
    "        self.zero_mask = Tensor()\n",
    "        self.pos_code = Tensor()\n",
    "        self.d = d\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.decoders = nn.Sequential()\n",
    "        for _ in range(decoder_num):\n",
    "            self.decoders.append(decoder(head_num, d, dk, dff))\n",
    "        self.last_linear = nn.Linear(d, self.vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def get_mask(self, sequence_len): # mask 机制\n",
    "        if not self.training:\n",
    "            if sequence_len != len(self.zero_mask):\n",
    "                self.zero_mask = torch.zeros(sequence_len, sequence_len)\n",
    "            return self.zero_mask\n",
    "        if sequence_len != len(self.mask):\n",
    "            self.mask = torch.zeros(sequence_len, sequence_len)\n",
    "            for i in range(sequence_len):\n",
    "                for j in range(sequence_len):\n",
    "                    if j>i: self.mask[i][j] = -1e9\n",
    "        return self.mask\n",
    "\n",
    "    def pos_encode(self, sequence_len): # 位置嵌入\n",
    "        if len(self.pos_code) == sequence_len:\n",
    "            return self.pos_code\n",
    "        self.pos_code = []\n",
    "        for pos in range(sequence_len): \n",
    "            buf = []\n",
    "            for i in range(self.d):\n",
    "                value = math.sin(pos/1e4**(i/self.d)) if i % 2 == 0  \\\n",
    "                        else math.cos(pos/1e4**((i-1)/self.d))\n",
    "                buf.append(value)\n",
    "            self.pos_code.append(torch.tensor(buf))\n",
    "        self.pos_code = torch.stack(self.pos_code)\n",
    "        return self.pos_code\n",
    "\n",
    "    # x 应是一个tensor，纵向是序列长度，横向是vocab size长度的one hot向量\n",
    "    def forward(self, x:Tensor):\n",
    "        sequence_len = x.shape[-2]\n",
    "        x = x @ self.last_linear.weight  # embedding, share weight matrix     \n",
    "        x = x * self.d**0.5 + self.pos_encode(sequence_len) # 位置嵌入\n",
    "        y, _ = self.decoders((x, self.get_mask(sequence_len)))\n",
    "        y = self.last_linear(y) \n",
    "        # 输出的每个 vector 表示对现在位置的下一个词的预测，表示未知的词的只有最后一个vector。\n",
    "        return y #self.softmax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10; Loss: 2.447510004043579\n",
      "Epoch: 20; Loss: 0.3105463683605194\n",
      "Epoch: 30; Loss: 0.060086172074079514\n",
      "Epoch: 40; Loss: 0.025429347530007362\n",
      "Epoch: 50; Loss: 0.00963953323662281\n",
      "Epoch: 60; Loss: 0.00402106624096632\n",
      "Epoch: 70; Loss: 0.002717043971642852\n",
      "Epoch: 80; Loss: 0.002148592844605446\n",
      "Epoch: 90; Loss: 0.00181028142105788\n",
      "Epoch: 100; Loss: 0.0015870328061282635\n"
     ]
    }
   ],
   "source": [
    "model = transformer(decoder_num=3, head_num=4, \n",
    "                    d = 256,  dk = 64,  dff = 512)\n",
    "\n",
    "def train(epoch:int):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.train() # 设置为训练模式\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "    for i in range(epoch):\n",
    "        loss = 0\n",
    "        for x, y in data:\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch: {i+1}; Loss: {loss.item()}')\n",
    "\n",
    "train(100)\n",
    "torch.save(model.state_dict(), 'weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "runner = transformer(decoder_num=3, head_num=4, \n",
    "                    d = 256,  dk = 64,  dff = 512)\n",
    "runner.load_state_dict(torch.load('weights.pth'))\n",
    "runner.eval()\n",
    "\n",
    "# print(tokenizer.decode(data.tokens[:10]))\n",
    "\n",
    "def run_model(input:str):\n",
    "    tokens = tokenizer.encode(input)[:-1]\n",
    "    pred, cnt = 101, 0\n",
    "    while pred!=102 and cnt<=500:\n",
    "        x = data.one_hot(tokens, 1)\n",
    "        pred = runner(x)[-1].argmax(dim = -1).item()\n",
    "\n",
    "        word = tokenizer.decode([pred])\n",
    "        if word.find('##') == -1 and word[0] not in string.punctuation:\n",
    "            print(' ' + word, end='')\n",
    "        else: print(word.replace('##', ''), end='')\n",
    "\n",
    "        tokens.append(pred)\n",
    "        cnt += 1\n",
    "        if cnt % 20 == 0:\n",
    "            print('')\n",
    "    # print(tokenizer.decode(tokens))\n",
    "\n",
    "# x, y = data[0]\n",
    "# tokenizer.decode(runner(x).argmax(dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 敏感性分析\n",
    "\n",
    "### 参数一\n",
    "decoder_num=3, head_num=4, d=256, dk=64, dff=512\n",
    "\n",
    "Adam, lr=0.0003, epoch=100\n",
    "\n",
    "Loss: 0.0015870328061282635\n",
    "\n",
    "准确输出文章全部内容\n",
    "\n",
    "### 参数二\n",
    "decoder_num=3, head_num=4, d=64, dk=16, dff=128\n",
    "\n",
    "Loss: 0.032027047127485275\n",
    "\n",
    "内容输出较为准确，有些许误差\n",
    "\n",
    "### 参数三\n",
    "decoder_num=2, head_num=2, d=32, dk=16, dff=64\n",
    "\n",
    "Loss: 0.51451575756073\n",
    "\n",
    "无法输出正常内容\n",
    "\n",
    "\n",
    "### 去除位置编码\n",
    "使用参数一\n",
    "\n",
    "Loss: 0.04319296404719353\n",
    "\n",
    "输出内容很不准确\n",
    "\n",
    "### 去除 mask\n",
    "使用参数一\n",
    "\n",
    "Loss: 0.0039401608519256115\n",
    "\n",
    "输出内容准确，怀疑是过拟合导致的\n",
    "\n",
    "使用参数二\n",
    "\n",
    "Loss: 0.0312423724681139\n",
    "\n",
    "虽然 Loss 和有 mask 时几乎一致，但输出内容很不准确\n",
    "\n",
    "### Q、K、V 共享线性层\n",
    "使用参数一\n",
    "\n",
    "Loss: 0.0018703739624470472\n",
    "\n",
    "输出内容非常准确\n",
    "\n",
    "使用参数二\n",
    "\n",
    "Loss: 0.03298478573560715\n",
    "\n",
    "输出内容非常准确\n",
    "\n",
    "### 去除 softmax 中的除以 $\\sqrt d$\n",
    "使用参数二\n",
    "\n",
    "Loss: 0.037089236080646515\n",
    "\n",
    "输出内容有很多误差，梯度消失\n",
    "\n",
    "### 去除位置编码前的乘以 $\\sqrt d$\n",
    "使用参数二\n",
    "\n",
    "Loss: 0.036799754947423935\n",
    "\n",
    "输出很准确，过拟合导致\n",
    "\n",
    "### 去除残差\n",
    "使用参数二\n",
    "\n",
    "Loss: 4.3070549964904785\n",
    "\n",
    "无法正常输出\n",
    "\n",
    "### 去除标准化\n",
    "使用参数二\n",
    "\n",
    "Loss: 0.0009859238052740693\n",
    "\n",
    "无法正常输出，原因是超级过拟合，当设置成训练模式时就可以了\n",
    "\n",
    "### 去除 ffn\n",
    "使用参数二\n",
    "\n",
    "Loss: 0.09225127100944519\n",
    "\n",
    "无法正常输出，欠拟合"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
